{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strength of visualization-python visuals tutorial\n",
    "* Original Notebook: https://www.kaggle.com/maheshdadhich/strength-of-visualization-python-visuals-tutorial\n",
    "* Data:\n",
    " * https://www.kaggle.com/c/nyc-taxi-trip-duration\n",
    " * https://www.kaggle.com/oscarleo/new-york-city-taxi-with-osrm\n",
    " * https://www.kaggle.com/mathijs/weather-data-in-new-york-city-2016\n",
    " \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble \n",
    "I am always told that there are no good visualization libraries in python and also felt the same way when I saw \"heads and tails\" R notebook with ggplot2 and tidy. A thought and wondered into my mind- is it really true ? are there no fancy visualization libaries in python? So I start exploring them and found that it's absolutely wrong. So **I will try to make this notebook as a visualization tutorial in python** and will be using data from NYC taxi competition to construct the visualizations and I will also be explaining in chronological order the way to interpret shown visualizations and how visualizations can provide beautiful ideas about new features. \n",
    "\n",
    "This notebook may also be used by beginers to understand analytical thinking, how each step will be guiding us to next step will be explaining thoroughly. We will have interpretation of the plot right below it and ideas we get by that figure. I will try to make it as comprehensive as possible and will try to cover all the following packages:\n",
    "\n",
    "| Serial No.  | Package     | Plots used in this kernel                          | Remark         \n",
    "| :----------:|:----------: | :-------------------------------------------------:|:-----------------------------:|\n",
    "|1| **Matplotlib** |1. vendor_id histogram, 2. store and fwdflag histrogram |Matplotlib is oldest and most widely used python visalization package, its a decade old but stil its the first name come to our mind when it comes to plotting. Many libaries are build on top of it, and uses its functions in the backend. Its style is very simple and that's the reason plotting is fast in this. It is used to create axis and design the layout for plotting using other libraries like seaborn.| \n",
    "|2| **Seaborn** |1.Voilin plot (passenger count vs trip duration), 2. Boxplots( Weekday vs trip duration, 3. tsplot (hours, weekday vs avg trip duration), 4. distplots of lat-long, and trip_duration |Seaborn is my favorite plotting library (Not at all a fan of house greyjoy though :P) Plots from this package are soothing to eyes. Its build as a wrapper on matplotlib and many matplotlib's function are also work with it.colors are amazing in this package's plots|\n",
    "|3|**Pandas**  | 1. Paraller coordinates (for cluster characteristics) | Pandas also offer many plotting functions and its also a package built on matplotlib, so you need to know matplotlib to twick the defaults of pandas. Its offers Alluvial plots (which are nowhere near what R offers as alluvial plots) which is used in this notebbok to show cluster characteristics.|\n",
    "|4|**Bokeh** |1. Time series plot (day of year vs avg trip duration) | Bokeh is one great package which offers interactive plots, you can use bokeh with other libraries like seaborn, data-shadder or holoviews, but bokeh its offers various different kind of plots. zoom, axis and interactive legends makes bokeh different than others|\n",
    "|5|**Folium** | 1.pickup locations in manhattan, 2. cluster's location in USA, 3. clusters location in manhattan | This package offers geographical-maps and that to are interactive in nature. This package offers different kind of terrains for maps- stemmer terrain, open street maps to name a few. you can place bubble at the locations, shift the zoom, and scroll the plot left-right up-down and add interactions, for example - cluster plots shown in this notebook offers information about clusters like number of vehicles going out, most frequently visited clusters etc. *kaggle started supporting this package during this competetion only* |\n",
    "|6|**Pygmaps** | 1. location visualizations 2. cluster visualizations | Pygmaps is available as archive package and can't even be installed using pip install command, but this package was the predeccesor of gamps package but offers few great interactions which even gmaps does't offer. for example scattering of cluster can be plotting with this one better than with gmaps. This package was way underdeveloped and developed version of it is know as gmaps yet, it was able to generate beautiful vizs. plots made my this package are best viewed in browsers.|\n",
    "|7|**Plotly**| 1.bubble plot |This is another great package which offers colorful visualizations, but some of these beautiful plots require to interact with plotly server so you need API key and it will call the API.|\n",
    "|8|**Gmaps**|*To be updated*|gmaps provide great features like- route features, and we all are too used to gmaps, so feel like home.|\n",
    "|9|**Ggplot2** | 1. Weather plots of NYC for given period | gglots are now available in python as well, and its kind of in developing state and documentaion is less of this package which makes it a little difficult but at the same time it provides very beautiful plots, so there is a tradeoff ;)|\n",
    "|10|**Basemaps**| *Will not be added in this kernel*|As ong as you are not developing any maps related library, there is no benefits of using basemaps. They offere many options but using them is difficult, due to lots of arguments, differnet style and less documentaions, and many lines of codes sometimes will be required to plot a map.|\n",
    "|11|**No package**| 1. heatmaps of NYC taxi traffic |Instead of depending on data-shadder, I tried plotting the heatmap of traffic data with a row image, you will get to knoe the basics of image processing( reading image, color schemes that's all :P ) and how such basic exercise can result in traffic heatmap|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a27209d6-8bc5-49ea-9189-d782e2b9c4f3",
    "_execution_state": "idle",
    "_uuid": "e92fdcb5637c0cc953be51957ed3bfcb2394d3f7"
   },
   "source": [
    "## About Competition \n",
    "In this competition we are asked to build a model to predict trip duration for given pick and drop lat-long. We will keep on analysing the data and extracting features as we go forward with this notebook. Let's start get our hands dirty with data.We will be using following datasets in this competition to generate a model and prociding visuals:\n",
    "\n",
    "| Serial No.  | Datasets used | Description |\n",
    "| :----------:|:----------: |:-----------------------------:|\n",
    "|1|NYC taxi train-test| Datasets provided as standard data for this competition.\n",
    "|2|NYC OSRM dataset| Dataset contains, fatest route, and second fatest route and path, trip duration information.\n",
    "|3|Weather data| Dataset contain, precipitation, snowfall etc for given time duration of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "68911a30-9d36-4e4f-a4ed-560fac85826d",
    "_uuid": "b5f740990545aa8ad4552e4af6b5629a07f87d31"
   },
   "source": [
    "##### Importing packages for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7796c42c-c271-47af-a17a-492772e34075",
    "_execution_state": "idle",
    "_uuid": "f60942eb1d22ed39ab9ccef7e14a3f51d28d8a6d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd  #pandas for using dataframe and reading csv \n",
    "import numpy as np   #numpy for vector operations and basic maths \n",
    "#import simplejson    #getting JSON in simplified format\n",
    "import urllib        #for url stuff\n",
    "#import gmaps       #for using google maps to visulalize places on maps\n",
    "import re            #for processing regular expressions\n",
    "import datetime      #for datetime operations\n",
    "import calendar      #for calendar for datetime operations\n",
    "import scipy         #for other dependancies\n",
    "from sklearn.cluster import KMeans # for doing K-means clustering\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from haversine import haversine # for calculating haversine distance\n",
    "import math          #for basic maths operations\n",
    "import seaborn as sns #for making plots\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # quell nasty seaborn depreciation warnings\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "%matplotlib inline\n",
    "import os  # for os commands\n",
    "from scipy.misc import imread, imresize, imsave  # for plots, needs Pillow\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "06b782a3-cade-47ab-af5d-4026ce11ac1d",
    "_uuid": "10dff8114b6dde38350ed094699ef68f02e96d60"
   },
   "source": [
    "### Reading and checking the head of training data and data from OSRM fastest route dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "19a05f83-765f-4f15-afa8-b62a07244754",
    "_execution_state": "idle",
    "_uuid": "0ca716bfb4bd2e10cb9891a89d4ad126595da323"
   },
   "outputs": [],
   "source": [
    "train_fr_1 = pd.read_csv('/data/data/newyork/new-york-city-taxi-with-osrm/fastest_routes_train_part_1.csv.gz')\n",
    "train_fr_2 = pd.read_csv('/data/data/newyork/new-york-city-taxi-with-osrm/fastest_routes_train_part_2.csv.gz')\n",
    "train_fr = pd.concat([train_fr_1, train_fr_2])\n",
    "train_fr_new = train_fr[['id', 'total_distance', 'total_travel_time', 'number_of_steps']]\n",
    "train_df = pd.read_csv('/data/data/newyork/nyc-taxi-trip-duration/train.csv.gz')\n",
    "train = pd.merge(train_df, train_fr_new, on = 'id', how = 'left')\n",
    "train_df = train.copy()\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "25133c1e-576a-461a-967c-fd48d31f64c6",
    "_uuid": "559656b78d38f1e137bf236b2565f79de41f575d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# checking if Ids are unique, \n",
    "train_data = train_df.copy()\n",
    "print(\"Number of columns and rows and columns are {} and {} respectively.\".format(train_data.shape[1], train_data.shape[0]))\n",
    "if train_data.id.nunique() == train_data.shape[0]:\n",
    "    print(\"Train ids are unique\")\n",
    "print(\"Number of Nulls: {}.\".format(train_data.isnull().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e2f7093d-9a02-4e8a-81b2-4d112046e1e3",
    "_uuid": "28856c384a3d78bcf06111c03e9b362cc886f059"
   },
   "source": [
    "### Lets visualize the trip duration given using log-scale distplot in sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5672b967-0e63-4aa4-a498-035d9cb66904",
    "_uuid": "5270bc575832e1ad2d1570341eb270b210e95f43",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "f, axes = plt.subplots(1, 1, figsize=(11, 7), sharex=True)\n",
    "sns.despine(left=True)\n",
    "sns.distplot(np.log(train_df['trip_duration'].values+1), axlabel = 'Log(trip_duration)', label = 'log(trip_duration)',\n",
    "             bins = 50, color=\"r\")\n",
    "plt.setp(axes, yticks=[])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "75ae720e-8826-430e-9d4d-368724e587ae",
    "_uuid": "d39d7ea5be673d03afd85ff9b597e772439c1898"
   },
   "source": [
    "**Findings**: It is clear with the above histrogram and kernel density plot that the trip-durations are like gaussian and few trips have very large duration, like ~350000 seconds which is 100 hours (which is weird, as long as it isn't a inter city taxi ride from NYC to SF or Alaska), while most of the trips are e^4 = 1 minute to e^8 ~ 60 minutes. and probably are taken inside manhattan or in new york only. Lets check the lat long distributions are then use them to have a heatmap kind of view of given lat-longs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "77275f15-384e-42de-a817-3857e5fd9193",
    "_uuid": "c19fb368721a5729135af5338fc836dac453bbc4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "f, axes = plt.subplots(2,2,figsize=(10, 10), sharex=False, sharey = False)#\n",
    "sns.despine(left=True)\n",
    "sns.distplot(train_df['pickup_latitude'].values, label = 'pickup_latitude',color=\"m\",bins = 100, ax=axes[0,0])\n",
    "sns.distplot(train_df['pickup_longitude'].values, label = 'pickup_longitude',color=\"m\",bins =100, ax=axes[0,1])\n",
    "sns.distplot(train_df['dropoff_latitude'].values, label = 'dropoff_latitude',color=\"m\",bins =100, ax=axes[1, 0])\n",
    "sns.distplot(train_df['dropoff_longitude'].values, label = 'dropoff_longitude',color=\"m\",bins =100, ax=axes[1, 1])\n",
    "plt.setp(axes, yticks=[])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eb9094ae-980e-47d8-a454-1afeaa0db043",
    "_uuid": "8106eaa2e9733d5ab8acc51a16dd47bb359b3c15"
   },
   "source": [
    "**Findings**: From the plot above it is clear that pick and drop latitude are centered around 40 to 41, and longitude are situated around -74 ton-73. we are not getting any histogram kind of plotswhen we are plotting lat- long as the distplot frunction of sns is getting affected by outliers, trips which are very far from each other like lat 32 to lat 44, are taking very long time, and affected this plot such that it is coming of as a spike. Let's remove those large duration trip by using a cap on lat-long and visulalize the distributions of latitude and longitude given to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "baabcfd4-3b28-45e7-9a6f-193614146796",
    "_uuid": "577dde4bbaa8ed22fe1c84bf7713247419bfaa72",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = train_df.loc[(train_df.pickup_latitude > 40.6) & (train_df.pickup_latitude < 40.9)]\n",
    "df = df.loc[(df.dropoff_latitude>40.6) & (df.dropoff_latitude < 40.9)]\n",
    "df = df.loc[(df.dropoff_longitude > -74.05) & (df.dropoff_longitude < -73.7)]\n",
    "df = df.loc[(df.pickup_longitude > -74.05) & (df.pickup_longitude < -73.7)]\n",
    "train_data_new = df.copy()\n",
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "f, axes = plt.subplots(2,2,figsize=(12, 12), sharex=False, sharey = False)#\n",
    "sns.despine(left=True)\n",
    "sns.distplot(train_data_new['pickup_latitude'].values, label = 'pickup_latitude',color=\"m\",bins = 100, ax=axes[0,0])\n",
    "sns.distplot(train_data_new['pickup_longitude'].values, label = 'pickup_longitude',color=\"g\",bins =100, ax=axes[0,1])\n",
    "sns.distplot(train_data_new['dropoff_latitude'].values, label = 'dropoff_latitude',color=\"m\",bins =100, ax=axes[1, 0])\n",
    "sns.distplot(train_data_new['dropoff_longitude'].values, label = 'dropoff_longitude',color=\"g\",bins =100, ax=axes[1, 1])\n",
    "plt.setp(axes, yticks=[])\n",
    "plt.tight_layout()\n",
    "print(df.shape[0], train_data.shape[0])\n",
    "temp = train_data.copy()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6fea7a83-ba17-413d-910d-8b65d5893ea9",
    "_uuid": "f19141c2fa40eb62f8fb1863e3a382eb2e23462d"
   },
   "source": [
    "As we put the following caps on lat-long\n",
    "- latitude should be between 40.6 to 40.9\n",
    "- longitude should be between -74.05 to -73.70 \n",
    "\n",
    "We get that the distribution spikes becomes as distribution in distplot (distplot is a histrogram plot in seaborn package), we can see that most of the trips are getting concentrated between these lat-long only. Lets plot them on an empty image and check what kind of a city map we are getting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5cee0fb9-dc39-4d7d-b95a-ac09025086bd",
    "_uuid": "88152d7bac3aa8c7c4a9477f4ae901698aba625d"
   },
   "source": [
    "## Heatmap of coordinates\n",
    "### Let's do basic image processing here \n",
    "We taken an empty image and make it color black so that we can see colors where the lat-longs are falling. To visualize we need to consider each point of this image as a point represented by lat-long, to achieved that we will bring the lat-long to image coordinate range and then take a summary of lat-long and their count, assign different color for different count range. Running next cell will result in beautiful visualization shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "06c673ff-7a35-40b8-8ba9-569526ad6bdb",
    "_uuid": "80e4445de060f92f57a7264713157484e9ffe08a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rgb = np.zeros((3000, 3500, 3), dtype=np.uint8)\n",
    "rgb[..., 0] = 0\n",
    "rgb[..., 1] = 0\n",
    "rgb[..., 2] = 0\n",
    "train_data_new['pick_lat_new'] = list(map(int, (train_data_new['pickup_latitude'] - (40.6000))*10000))\n",
    "train_data_new['drop_lat_new'] = list(map(int, (train_data_new['dropoff_latitude'] - (40.6000))*10000))\n",
    "train_data_new['pick_lon_new'] = list(map(int, (train_data_new['pickup_longitude'] - (-74.050))*10000))\n",
    "train_data_new['drop_lon_new'] = list(map(int,(train_data_new['dropoff_longitude'] - (-74.050))*10000))\n",
    "\n",
    "summary_plot = pd.DataFrame(train_data_new.groupby(['pick_lat_new', 'pick_lon_new'])['id'].count())\n",
    "\n",
    "summary_plot.reset_index(inplace = True)\n",
    "summary_plot.head(120)\n",
    "lat_list = summary_plot['pick_lat_new'].unique()\n",
    "for i in lat_list:\n",
    "    lon_list = summary_plot.loc[summary_plot['pick_lat_new']==i]['pick_lon_new'].tolist()\n",
    "    unit = summary_plot.loc[summary_plot['pick_lat_new']==i]['id'].tolist()\n",
    "    for j in lon_list:\n",
    "        a = unit[lon_list.index(j)]\n",
    "        if (a//50) >0:\n",
    "            rgb[i][j][0] = 255\n",
    "            rgb[i,j, 1] = 255\n",
    "            rgb[i,j, 2] = 0\n",
    "        elif (a//10)>0:\n",
    "            rgb[i,j, 0] = 0\n",
    "            rgb[i,j, 1] = 255\n",
    "            rgb[i,j, 2] = 0\n",
    "        else:\n",
    "            rgb[i,j, 0] = 255\n",
    "            rgb[i,j, 1] = 0\n",
    "            rgb[i,j, 2] = 0\n",
    "fig, ax = plt.subplots(nrows=1,ncols=1,figsize=(14,20))\n",
    "ax.imshow(rgb, cmap = 'hot')\n",
    "ax.set_axis_off() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "036e9d7b-f888-404b-b672-6971fe6026f4",
    "_uuid": "3d5514e7e0f943b57300e8f51320c35c270ca63a"
   },
   "source": [
    "From the heatmap kind of image above:\n",
    "- Red points signifies that 1-10 trips in the given data have that point as pickup point\n",
    "- Green points signifies that more than 10-50 trips in the given data have that point as pickup point \n",
    "- Yellow points signifies that more than 50+ trips in the given data have that point as pickup point\n",
    "\n",
    "Clearly the whole manhattan is yellow colored and with few green points as well, that shows that in manhatten most of the\n",
    "trips are getting originated. This is basic way in which you can plot large geospatial data in a empty image without being dependent on any package. But is you hate image processing, you can use [data shader](http://datashader.readthedocs.io/en/latest/), which is a package which is used to show billions of datapoints on a image, they also uses the similar approach with different color gradient.\n",
    "Thought I will also show the same plot with a sample data of 1000 trips on pygmaps in next few cell. it will generate a HTML in output and user has to open that HTML in browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4aa49689-e4ad-442f-86b7-ed9ed715586b",
    "_uuid": "fef671b57eac8bfed5835be27c89e12151398f2d"
   },
   "source": [
    "## Let's define few functions to unfold features in the given data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ff9b2a49-bbab-471a-9a59-acf6a88b8a7f",
    "_uuid": "aebf77a330cee7fca11cf15cedb2dcf6aa61ab2a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def haversine_(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"function to calculate haversine distance between two co-ordinates\"\"\"\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n",
    "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n",
    "    return(h)\n",
    "\n",
    "def manhattan_distance_pd(lat1, lng1, lat2, lng2):\n",
    "    \"\"\"function to calculate manhatten distance between pick_drop\"\"\"\n",
    "    a = haversine_(lat1, lng1, lat1, lng2)\n",
    "    b = haversine_(lat1, lng1, lat2, lng1)\n",
    "    return a + b\n",
    "\n",
    "import math\n",
    "def bearing_array(lat1, lng1, lat2, lng2):\n",
    "    \"\"\" function was taken from beluga's notebook as this function works on array\n",
    "    while my function used to work on individual elements and was noticably slow\"\"\"\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lng_delta_rad = np.radians(lng2 - lng1)\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    y = np.sin(lng_delta_rad) * np.cos(lat2)\n",
    "    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n",
    "    return np.degrees(np.arctan2(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c470141f-5c00-4d8a-92f4-48bb2a0f4ddc",
    "_uuid": "3422dfc6534d02bdee5c5fc6fb35bc37b0442b95"
   },
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9feeabe5-5c88-4814-b235-a2103471336c",
    "_execution_state": "idle",
    "_uuid": "b587165e38bc01930bdea5e85cc511b74026205d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = temp.copy()\n",
    "train_data['pickup_datetime'] = pd.to_datetime(train_data.pickup_datetime)\n",
    "train_data.loc[:, 'pick_month'] = train_data['pickup_datetime'].dt.month\n",
    "train_data.loc[:, 'hour'] = train_data['pickup_datetime'].dt.hour\n",
    "train_data.loc[:, 'week_of_year'] = train_data['pickup_datetime'].dt.weekofyear\n",
    "train_data.loc[:, 'day_of_year'] = train_data['pickup_datetime'].dt.dayofyear\n",
    "train_data.loc[:, 'day_of_week'] = train_data['pickup_datetime'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5cf8b243-ad46-417d-8ee6-c95f387f4bd6",
    "_uuid": "f94713af786603a5dac6d59f5ba04d9aaa191cf5"
   },
   "source": [
    "### Lets call the above defined functions and extracts the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5918ba49-9c89-4e30-ad2f-6b27baf0c980",
    "_uuid": "077ee862708e9bcf6005cc007207c7f161c3b623",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.loc[:,'hvsine_pick_drop'] = haversine_(train_data['pickup_latitude'].values, train_data['pickup_longitude'].values, train_data['dropoff_latitude'].values, train_data['dropoff_longitude'].values)\n",
    "train_data.loc[:,'manhtn_pick_drop'] = manhattan_distance_pd(train_data['pickup_latitude'].values, train_data['pickup_longitude'].values, train_data['dropoff_latitude'].values, train_data['dropoff_longitude'].values)\n",
    "train_data.loc[:,'bearing'] = bearing_array(train_data['pickup_latitude'].values, train_data['pickup_longitude'].values, train_data['dropoff_latitude'].values, train_data['dropoff_longitude'].values)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "35702004-623c-4b24-8164-993966432ce2",
    "_execution_state": "idle",
    "_uuid": "b7d31da42b7ea3eddfaf593d1bd80481de783f2d"
   },
   "source": [
    "## Exploring new features\n",
    "- Let's check the average time taken by two different vendors vs weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d3e606c2-d35a-4d99-ac06-4f3cae5fdf70",
    "_uuid": "41f0f75207b042ba6aa08e5820b772c020602e6a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_wdays_avg_duration = pd.DataFrame(train_data.groupby(['vendor_id','day_of_week'])['trip_duration'].mean())\n",
    "summary_wdays_avg_duration.reset_index(inplace = True)\n",
    "\n",
    "summary_wdays_avg_duration['unit']=1\n",
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "sns.set_context(\"poster\")\n",
    "sns.tsplot(data=summary_wdays_avg_duration, time=\"day_of_week\", unit = \"unit\", condition=\"vendor_id\", value=\"trip_duration\")\n",
    "sns.despine(bottom = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ec7cb6c5-e3ff-4468-948b-de01c9d5e7ec",
    "_uuid": "85196694fd65ab699f65a394e1c7485dd1802006"
   },
   "source": [
    "**Findings**:\n",
    "It's clear that the vendor 1 is taking more time than vendor 2 on all the days of the week, we can also subset dataframe based on month and that will also give us the same results. the difference between average time taken by vendor 1 is ~250 seconds more than vendor 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c766fca2-8704-48ae-912b-e8b56a2964a9",
    "_uuid": "3bdb7425c64fabca58c3fb141be46645ffab5659"
   },
   "source": [
    "### Violin Plot\n",
    "- Violin plot can be made using seaborn package in python and with split\n",
    "- here we are using them to check the distributions, and horizontal lines inside them shows the quartiles\n",
    "- green one is vendor 1 and red one is vendor 2 and trip_duration is plaotted on log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9b3c7a18-f67c-4b7f-bd6e-ed97aea1201d",
    "_uuid": "9b8e1fd0465066aff04819a5625bd556732c2730",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", palette=\"pastel\", color_codes=True)\n",
    "sns.set_context(\"poster\")\n",
    "train_data2 = train_data.copy()\n",
    "train_data2['trip_duration']= np.log(train_data['trip_duration'])\n",
    "sns.violinplot(x=\"passenger_count\", y=\"trip_duration\", hue=\"vendor_id\", data=train_data2, split=True,\n",
    "               inner=\"quart\",palette={1: \"g\", 2: \"r\"})\n",
    "\n",
    "sns.despine(left=True)\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fce82ff8-844f-44c0-9ee3-099fa8384285",
    "_uuid": "6a833884c13d4cf9c0340357ceb1c7d28fd939b6"
   },
   "source": [
    "**Findings**\n",
    "- There are trips for both the vendor with zeros passengers and few of these trips have negative time as well, I don't understand how can a taxi trip have negative time, possibly they aren't right datapoints to train the model, we would remove them before making model\n",
    "- Trips with zero passengers can be trips when taxi is called to a particular location and the customer is charged for getting the taxi there, that is one possible explanation.\n",
    "- Distributions are similar for both the vendors\n",
    "- There are a lot less number of trips with passanger count 7,8 and 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "626a0f85-f926-42b9-9eb0-e9e439ddbe0f",
    "_uuid": "e8999d9207ffdd5635ba2b46dc150e44269f2bfb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\")\n",
    "sns.set_context(\"poster\")\n",
    "sns.boxplot(x=\"day_of_week\", y=\"trip_duration\", hue=\"vendor_id\", data=train_data, palette=\"PRGn\")\n",
    "plt.ylim(0, 6000)\n",
    "sns.despine(offset=10, trim=True)\n",
    "train_data.trip_duration.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d63a59ad-5fc6-4488-9730-66a1c7ef6794",
    "_uuid": "c00e51d8c81c31673bdae722798daf9a36c9329f"
   },
   "source": [
    "**Findings**\n",
    "- From the boxplot above we can see that 75%ile of avg trip duration on sunday(0) and saturday(6) is less than 2000 seconds. i.e. around 33 minutes\n",
    "- Time taken by Monday, Tuesday, Wednesday and Thursday are greater than rest of the days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "94b79a99-a20b-4757-9c16-21ac30a4e86e",
    "_uuid": "e5628b1f0960d39e5a9b5c84741d92f07dac3524",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_hour_duration = pd.DataFrame(train_data.groupby(['day_of_week','hour'])['trip_duration'].mean())\n",
    "summary_hour_duration.reset_index(inplace = True)\n",
    "summary_hour_duration['unit']=1\n",
    "sns.set(style=\"white\", palette=\"muted\", color_codes=False)\n",
    "sns.set_context(\"poster\")\n",
    "sns.tsplot(data=summary_hour_duration, time=\"hour\", unit = \"unit\", condition=\"day_of_week\", value=\"trip_duration\")\n",
    "sns.despine(bottom = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f75425a8-2f2d-428c-bc7a-6224bf8f115a",
    "_uuid": "27fb3a8014aca2998ed684a9a6bff4ca4985b40e"
   },
   "source": [
    "**Findings**\n",
    "- Its clear from the above plot that on day 0, that is sunday and day day 6 that is saturday, the trip duration is a lot less than all the weekdays in 5AM to 15AM time. \n",
    "- See this, on  Saturday around midnight, the rides are taking far more than usual time, this is obvious though now verified  using given data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ccd6e1bf-cd1f-4846-b8ac-224399754230",
    "_uuid": "61fcefb7ba1ebab9f81b2577d0f5bb87e2e776ed"
   },
   "source": [
    "# Cluster analysis and visualization\n",
    "\n",
    "Now I will be using folium package for visualizations and we will be visualizing clusters on maps. Folium is map based interactive package and it's open source as well. Its based on leaflet js. It is  interactive in nature, balloons on the map can be click and they will show characteristics of that clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "990ec289-afbc-471c-acd2-b8e4a6d81d7d",
    "_uuid": "d87f7f0a84271a6afbdfb1d47a37f48a00ece1d8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_cluster(df, k):\n",
    "    \"\"\"function to assign clusters \"\"\"\n",
    "    df_pick = df[['pickup_longitude','pickup_latitude']]\n",
    "    df_drop = df[['dropoff_longitude','dropoff_latitude']]\n",
    "    #df = df.dropna()\n",
    "    init = np.array([[ -73.98737616,   40.72981533],\n",
    "       [-121.93328857,   37.38933945],\n",
    "       [ -73.78423222,   40.64711269],\n",
    "       [ -73.9546417 ,   40.77377538],\n",
    "       [ -66.84140269,   36.64537175],\n",
    "       [ -73.87040541,   40.77016484],\n",
    "       [ -73.97316185,   40.75814346],\n",
    "       [ -73.98861094,   40.7527791 ],\n",
    "       [ -72.80966949,   51.88108444],\n",
    "       [ -76.99779701,   38.47370625],\n",
    "       [ -73.96975298,   40.69089596],\n",
    "       [ -74.00816622,   40.71414939],\n",
    "       [ -66.97216034,   44.37194443],\n",
    "       [ -61.33552933,   37.85105133],\n",
    "       [ -73.98001393,   40.7783577 ],\n",
    "       [ -72.00626526,   43.20296402],\n",
    "       [ -73.07618713,   35.03469086],\n",
    "       [ -73.95759366,   40.80316361],\n",
    "       [ -79.20167796,   41.04752096],\n",
    "       [ -74.00106031,   40.73867723]])\n",
    "    k_means_pick = KMeans(n_clusters=k, init=init, n_init=1)\n",
    "    k_means_pick.fit(df_pick)\n",
    "    clust_pick = k_means_pick.labels_\n",
    "    df['label_pick'] = clust_pick.tolist()\n",
    "    df['label_drop'] = k_means_pick.predict(df_drop)\n",
    "    return df, k_means_pick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c50de5dc-7c2f-44da-b7f2-53ae60cd501e",
    "_uuid": "1ebf8e93142edb678f9d83c1a5ea94050d25af36",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_cl, k_means = assign_cluster(train_data, 20)  # make it 100 when extracting features \n",
    "centroid_pickups = pd.DataFrame(k_means.cluster_centers_, columns = ['centroid_pick_long', 'centroid_pick_lat'])\n",
    "centroid_dropoff = pd.DataFrame(k_means.cluster_centers_, columns = ['centroid_drop_long', 'centroid_drop_lat'])\n",
    "centroid_pickups['label_pick'] = centroid_pickups.index\n",
    "centroid_dropoff['label_drop'] = centroid_dropoff.index\n",
    "train_cl = pd.merge(train_cl, centroid_pickups, how='left', on=['label_pick'])\n",
    "train_cl = pd.merge(train_cl, centroid_dropoff, how='left', on=['label_drop'])\n",
    "train_cl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d9e5fc9c-f3b3-4442-9788-8c0c308d36dc",
    "_uuid": "47124b0a1fe31ba1ef9223fd586754cc3732322b"
   },
   "source": [
    "# Cluster related features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f7ee54b2-32fd-47b8-9c05-6a105337eca4",
    "_uuid": "471187d496fbcd59ccd6995bf673ef5932462ac8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_cl.loc[:,'hvsine_pick_cent_p'] = haversine_(train_cl['pickup_latitude'].values, train_cl['pickup_longitude'].values, train_cl['centroid_pick_lat'].values, train_cl['centroid_pick_long'].values)\n",
    "train_cl.loc[:,'hvsine_drop_cent_d'] = haversine_(train_cl['dropoff_latitude'].values, train_cl['dropoff_longitude'].values, train_cl['centroid_drop_lat'].values, train_cl['centroid_drop_long'].values)\n",
    "train_cl.loc[:,'hvsine_cent_p_cent_d'] = haversine_(train_cl['centroid_pick_lat'].values, train_cl['centroid_pick_long'].values, train_cl['centroid_drop_lat'].values, train_cl['centroid_drop_long'].values)\n",
    "train_cl.loc[:,'manhtn_pick_cent_p'] = manhattan_distance_pd(train_cl['pickup_latitude'].values, train_cl['pickup_longitude'].values, train_cl['centroid_pick_lat'].values, train_cl['centroid_pick_long'].values)\n",
    "train_cl.loc[:,'manhtn_drop_cent_d'] = manhattan_distance_pd(train_cl['dropoff_latitude'].values, train_cl['dropoff_longitude'].values, train_cl['centroid_drop_lat'].values, train_cl['centroid_drop_long'].values)\n",
    "train_cl.loc[:,'manhtn_cent_p_cent_d'] = manhattan_distance_pd(train_cl['centroid_pick_lat'].values, train_cl['centroid_pick_long'].values, train_cl['centroid_drop_lat'].values, train_cl['centroid_drop_long'].values)\n",
    "\n",
    "train_cl.loc[:,'bearing_pick_cent_p'] = bearing_array(train_cl['pickup_latitude'].values, train_cl['pickup_longitude'].values, train_cl['centroid_pick_lat'].values, train_cl['centroid_pick_long'].values)\n",
    "train_cl.loc[:,'bearing_drop_cent_p'] = bearing_array(train_cl['dropoff_latitude'].values, train_cl['dropoff_longitude'].values, train_cl['centroid_drop_lat'].values, train_cl['centroid_drop_long'].values)\n",
    "train_cl.loc[:,'bearing_cent_p_cent_d'] = bearing_array(train_cl['centroid_pick_lat'].values, train_cl['centroid_pick_long'].values, train_cl['centroid_drop_lat'].values, train_cl['centroid_drop_long'].values)\n",
    "train_cl['speed_hvsn'] = train_cl.hvsine_pick_drop/train_cl.total_travel_time\n",
    "train_cl['speed_manhtn'] = train_cl.manhtn_pick_drop/train_cl.total_travel_time\n",
    "train_cl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8dff96d6-650d-445d-939c-82526ce16e86",
    "_uuid": "614a3310e314226a073abe9b2c29d92d2a8289cb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_summary(sum_df):\n",
    "    \"\"\"function to calculate summary of given list of clusters \"\"\"\n",
    "    #agg_func = {'trip_duration':'mean','label_drop':'count','bearing':'mean','id':'count'} # that's how you use agg function with groupby\n",
    "    summary_avg_time = pd.DataFrame(sum_df.groupby('label_pick')['trip_duration'].mean())\n",
    "    summary_avg_time.reset_index(inplace = True)\n",
    "    summary_pref_clus = pd.DataFrame(sum_df.groupby(['label_pick', 'label_drop'])['id'].count())\n",
    "    summary_pref_clus = summary_pref_clus.reset_index()\n",
    "    summary_pref_clus = summary_pref_clus.loc[summary_pref_clus.groupby('label_pick')['id'].idxmax()]\n",
    "    summary =pd.merge(summary_avg_time, summary_pref_clus, how = 'left', on = 'label_pick')\n",
    "    summary = summary.rename(columns={'trip_duration':'avg_triptime'})\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e9f43aaf-a841-4461-8243-488ca175f868",
    "_uuid": "5847cae12d6afaa38d696ccf1e5f1f13f784b8e7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "def show_fmaps(train_data, path=1):\n",
    "    \"\"\"function to generate map and add the pick up and drop coordinates\n",
    "    1. Path = 1 : Join pickup (blue) and drop(red) using a straight line\n",
    "    \"\"\"\n",
    "    full_data = train_data\n",
    "    summary_full_data = pd.DataFrame(full_data.groupby('label_pick')['id'].count())\n",
    "    summary_full_data.reset_index(inplace = True)\n",
    "    summary_full_data = summary_full_data.loc[summary_full_data['id']>70000]\n",
    "    map_1 = folium.Map(location=[40.767937, -73.982155], zoom_start=10,tiles='Stamen Toner') # manually added centre\n",
    "    new_df = train_data.loc[train_data['label_pick'].isin(summary_full_data.label_pick.tolist())].sample(50)\n",
    "    new_df.reset_index(inplace = True, drop = True)\n",
    "    for i in range(new_df.shape[0]):\n",
    "        pick_long = new_df.loc[new_df.index ==i]['pickup_longitude'].values[0]\n",
    "        pick_lat = new_df.loc[new_df.index ==i]['pickup_latitude'].values[0]\n",
    "        dest_long = new_df.loc[new_df.index ==i]['dropoff_longitude'].values[0]\n",
    "        dest_lat = new_df.loc[new_df.index ==i]['dropoff_latitude'].values[0]\n",
    "        folium.Marker([pick_lat, pick_long]).add_to(map_1)\n",
    "        folium.Marker([dest_lat, dest_long]).add_to(map_1)\n",
    "    return map_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "00920f3e-f8cb-47bd-b643-b95ceb521392",
    "_uuid": "aa7b5c26a499e527b30cc6a8811e2dbda8965cca",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clusters_map(clus_data, full_data, tile = 'OpenStreetMap', sig = 0, zoom = 12, circle = 0, radius_ = 30):\n",
    "    \"\"\" function to plot clusters on map\"\"\"\n",
    "    map_1 = folium.Map(location=[40.767937, -73.982155], zoom_start=zoom,tiles= tile) # 'Mapbox' 'Stamen Toner'\n",
    "    summary_full_data = pd.DataFrame(full_data.groupby('label_pick')['id'].count())\n",
    "    summary_full_data.reset_index(inplace = True)\n",
    "    if sig == 1:\n",
    "        summary_full_data = summary_full_data.loc[summary_full_data['id']>70000]\n",
    "    sig_cluster = summary_full_data['label_pick'].tolist()\n",
    "    clus_summary = cluster_summary(full_data)\n",
    "    for i in sig_cluster:\n",
    "        pick_long = clus_data.loc[clus_data.index ==i]['centroid_pick_long'].values[0]\n",
    "        pick_lat = clus_data.loc[clus_data.index ==i]['centroid_pick_lat'].values[0]\n",
    "        clus_no = clus_data.loc[clus_data.index ==i]['label_pick'].values[0]\n",
    "        most_visited_clus = clus_summary.loc[clus_summary['label_pick']==i]['label_drop'].values[0]\n",
    "        avg_triptime = clus_summary.loc[clus_summary['label_pick']==i]['avg_triptime'].values[0]\n",
    "        pop = 'cluster = '+str(clus_no)+' & most visited cluster = ' +str(most_visited_clus) +' & avg triptime from this cluster =' + str(avg_triptime)\n",
    "        if circle == 1:\n",
    "            folium.CircleMarker(location=[pick_lat, pick_long], radius=radius_,\n",
    "                    color='#F08080',\n",
    "                    fill_color='#3186cc', popup=pop).add_to(map_1)\n",
    "        folium.Marker([pick_lat, pick_long], popup=pop).add_to(map_1)\n",
    "    return map_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cdbafb09-e192-42c8-955b-4a81ded9e733",
    "_uuid": "576da17fcfd4ea175afcc880141c85f9fb4e998c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "osm = show_fmaps(train_data, path=1)\n",
    "osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "409e5b18-1452-45dc-b358-c1f99c390428",
    "_uuid": "4dd881f91b4c60ed0001e12410eb1cd4069183ea"
   },
   "source": [
    "### Findings\n",
    "Clusters with more than 70k pickups are taken for plotting this map, and they are covering the most of the rides, more than ~80%, so then plotting a sample of them on this maps shows that most of the rides are started from manhattan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a1095ce4-86ac-4051-a582-49a9cde567a5",
    "_uuid": "bf578923cbf04f12364651cb2ffdae7bcf0b422d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clus_map = clusters_map(centroid_pickups, train_cl, sig =0, zoom =3.2, circle =1, tile = 'Stamen Terrain')\n",
    "clus_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bc2013ea-8ec3-4452-9b39-3933656c0cc2",
    "_uuid": "f42402571dd4401b5f217560a9fbe46b46b5ec4a"
   },
   "source": [
    "### Findings\n",
    "1. One cluster is formed in california and and one is very far in north, so few people taking rides from california as well, and I guess that's why few rides have very long trip duration.\n",
    "2. Few clusters are getting centred in sea, its funny but few people are taking ride on the sea as well. :P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "afb975f3-13d8-4819-bfd1-c1ad999a4bef",
    "_uuid": "5f728c8d32fd462320991af4eff6ccf0e1bbcbdf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clus_map_sig = clusters_map(centroid_pickups, train_cl, sig =1, circle =1)\n",
    "clus_map_sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a35fdee4-a6ac-4479-8572-022d993e857f",
    "_uuid": "f82b898c01aa52d6381e56f74dad2e5dc70e6029"
   },
   "source": [
    "**plots are interactive click on balloon as check out the characteristics of each cluster - **\n",
    "1. Cluster number \n",
    "2. Most frequently visited cluster from clicked cluster\n",
    "3. Avg triptime of rides started from this cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "f705941a-7634-4595-b177-5887b98b013a",
    "_uuid": "38df164eb72956f090ed2b0a3718acbdc1af9175"
   },
   "outputs": [],
   "source": [
    "#train_cl.to_csv(\"train_features_md.csv\")\n",
    "# Let's make test features as well \n",
    "test_df = pd.read_csv('/data/data/newyork/nyc-taxi-trip-duration/test.csv.gz')\n",
    "test_fr = pd.read_csv('/data/data/newyork/new-york-city-taxi-with-osrm/fastest_routes_test.csv.gz')\n",
    "test_fr_new = test_fr[['id', 'total_distance', 'total_travel_time', 'number_of_steps']]\n",
    "test_df = pd.merge(test_df, test_fr_new, on = 'id', how = 'left')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "62f9167e-986a-4073-906f-80f01c17f94a",
    "_uuid": "11884f428ed107ffc87c907cacf393d9b6c32979",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = test_df.copy()\n",
    "test_data['pickup_datetime'] = pd.to_datetime(test_data.pickup_datetime)\n",
    "test_data.loc[:, 'pick_month'] = test_data['pickup_datetime'].dt.month\n",
    "test_data.loc[:, 'hour'] = test_data['pickup_datetime'].dt.hour\n",
    "test_data.loc[:, 'week_of_year'] = test_data['pickup_datetime'].dt.weekofyear\n",
    "test_data.loc[:, 'day_of_year'] = test_data['pickup_datetime'].dt.dayofyear\n",
    "test_data.loc[:, 'day_of_week'] = test_data['pickup_datetime'].dt.dayofweek\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "85f87cd6-89c4-42b5-9fde-679df43b3db1",
    "_uuid": "925dbf7d02acf7094ad1ff579dc50038a4cbe05f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data.loc[:,'hvsine_pick_drop'] = haversine_(test_data['pickup_latitude'].values, test_data['pickup_longitude'].values, test_data['dropoff_latitude'].values, test_data['dropoff_longitude'].values)\n",
    "test_data.loc[:,'manhtn_pick_drop'] = manhattan_distance_pd(test_data['pickup_latitude'].values, test_data['pickup_longitude'].values, test_data['dropoff_latitude'].values, test_data['dropoff_longitude'].values)\n",
    "test_data.loc[:,'bearing'] = bearing_array(test_data['pickup_latitude'].values, test_data['pickup_longitude'].values, test_data['dropoff_latitude'].values, test_data['dropoff_longitude'].values)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d35b44de-0e36-4930-b93a-2ca5e93277b7",
    "_uuid": "9220aceed9a2297bb48369781db2c395fcf794c6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['label_pick'] = k_means.predict(test_data[['pickup_longitude','pickup_latitude']])\n",
    "test_data['label_drop'] = k_means.predict(test_data[['dropoff_longitude','dropoff_latitude']])\n",
    "test_cl = pd.merge(test_data, centroid_pickups, how='left', on=['label_pick'])\n",
    "test_cl = pd.merge(test_cl, centroid_dropoff, how='left', on=['label_drop'])\n",
    "#test_cl.head()\n",
    "test_cl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9ae870fc-1e3e-4376-9852-97d22c0362ec",
    "_uuid": "906d6e0346ae11b3094d528641ffd9c35263459b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_cl.loc[:,'hvsine_pick_cent_p'] = haversine_(test_cl['pickup_latitude'].values, test_cl['pickup_longitude'].values, test_cl['centroid_pick_lat'].values, test_cl['centroid_pick_long'].values)\n",
    "test_cl.loc[:,'hvsine_drop_cent_d'] = haversine_(test_cl['dropoff_latitude'].values, test_cl['dropoff_longitude'].values, test_cl['centroid_drop_lat'].values, test_cl['centroid_drop_long'].values)\n",
    "test_cl.loc[:,'hvsine_cent_p_cent_d'] = haversine_(test_cl['centroid_pick_lat'].values, test_cl['centroid_pick_long'].values, test_cl['centroid_drop_lat'].values, test_cl['centroid_drop_long'].values)\n",
    "test_cl.loc[:,'manhtn_pick_cent_p'] = manhattan_distance_pd(test_cl['pickup_latitude'].values, test_cl['pickup_longitude'].values, test_cl['centroid_pick_lat'].values, test_cl['centroid_pick_long'].values)\n",
    "test_cl.loc[:,'manhtn_drop_cent_d'] = manhattan_distance_pd(test_cl['dropoff_latitude'].values, test_cl['dropoff_longitude'].values, test_cl['centroid_drop_lat'].values, test_cl['centroid_drop_long'].values)\n",
    "test_cl.loc[:,'manhtn_cent_p_cent_d'] = manhattan_distance_pd(test_cl['centroid_pick_lat'].values, test_cl['centroid_pick_long'].values, test_cl['centroid_drop_lat'].values, test_cl['centroid_drop_long'].values)\n",
    "\n",
    "test_cl.loc[:,'bearing_pick_cent_p'] = bearing_array(test_cl['pickup_latitude'].values, test_cl['pickup_longitude'].values, test_cl['centroid_pick_lat'].values, test_cl['centroid_pick_long'].values)\n",
    "test_cl.loc[:,'bearing_drop_cent_p'] = bearing_array(test_cl['dropoff_latitude'].values, test_cl['dropoff_longitude'].values, test_cl['centroid_drop_lat'].values, test_cl['centroid_drop_long'].values)\n",
    "test_cl.loc[:,'bearing_cent_p_cent_d'] = bearing_array(test_cl['centroid_pick_lat'].values, test_cl['centroid_pick_long'].values, test_cl['centroid_drop_lat'].values, test_cl['centroid_drop_long'].values)\n",
    "test_cl['speed_hvsn'] = test_cl.hvsine_pick_drop/test_cl.total_travel_time\n",
    "test_cl['speed_manhtn'] = test_cl.manhtn_pick_drop/test_cl.total_travel_time\n",
    "test_cl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3736aba0-ee16-45fd-8357-51c31f753f96",
    "_uuid": "833318e61b59683352bee0fcd9d7248b57a26b15"
   },
   "source": [
    "# XGB Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anmerkung: [XGBoost](https://xgboost.readthedocs.io/) ist eine populäre Implementation von Gradient Boosting für Classification und Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7f0b69fa-f279-42fa-ab87-debb8574b2e5",
    "_uuid": "e3f6d5449af413f035f579c06103a58099f03af9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "561c76bb-88e4-496e-aadc-b7c03984dbb7",
    "_uuid": "14f14b829e307978cb03fab5d00d3717273dfe97",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets Add PCA features in the model, reference Beluga's PCA\n",
    "train = train_cl\n",
    "test = test_cl\n",
    "coords = np.vstack((train[['pickup_latitude', 'pickup_longitude']].values,\n",
    "                    train[['dropoff_latitude', 'dropoff_longitude']].values,\n",
    "                    test[['pickup_latitude', 'pickup_longitude']].values,\n",
    "                    test[['dropoff_latitude', 'dropoff_longitude']].values))\n",
    "\n",
    "pca = PCA().fit(coords)\n",
    "train['pickup_pca0'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 0]\n",
    "train['pickup_pca1'] = pca.transform(train[['pickup_latitude', 'pickup_longitude']])[:, 1]\n",
    "train['dropoff_pca0'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\n",
    "train['dropoff_pca1'] = pca.transform(train[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\n",
    "test['pickup_pca0'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 0]\n",
    "test['pickup_pca1'] = pca.transform(test[['pickup_latitude', 'pickup_longitude']])[:, 1]\n",
    "test['dropoff_pca0'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\n",
    "test['dropoff_pca1'] = pca.transform(test[['dropoff_latitude', 'dropoff_longitude']])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "11563557-0c76-40b8-a5ee-d94a18067646",
    "_uuid": "11a88ca70b58527394a3caa4e729f048ef33bd5b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['store_and_fwd_flag_int'] = np.where(train['store_and_fwd_flag']=='N', 0, 1)\n",
    "test['store_and_fwd_flag_int'] = np.where(test['store_and_fwd_flag']=='N', 0, 1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1ee798f6-d550-45f5-93c9-c267d5220876",
    "_uuid": "829dc3e69b454619f7a287b0b641170801d0c145",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = list(train.columns)\n",
    "print(\"Difference of features in train and test are {}\".format(np.setdiff1d(train.columns, test.columns)))\n",
    "print(\"\")\n",
    "do_not_use_for_training = ['id', 'pickup_datetime', 'dropoff_datetime', 'trip_duration', 'store_and_fwd_flag']\n",
    "feature_names = [f for f in train.columns if f not in do_not_use_for_training]\n",
    "print(\"We will be using following features for training {}.\".format(feature_names))\n",
    "print(\"\")\n",
    "print(\"Total number of features are {}.\".format(len(feature_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e46d9178-af6e-45aa-810b-ba649eca9b21",
    "_uuid": "f972f3eda87f3676e8f2ebcb88fb6d3a640480cd",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.log(train['trip_duration'].values + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e0da8416-d76f-4dc3-b8da-cbbed6269f72",
    "_uuid": "f678acd8eeb6673edea5ce5a91f6010daf1bd302",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtr, Xv, ytr, yv = train_test_split(train[feature_names].values, y, test_size=0.2, random_state=1987)\n",
    "dtrain = xgb.DMatrix(Xtr, label=ytr)\n",
    "dvalid = xgb.DMatrix(Xv, label=yv)\n",
    "dtest = xgb.DMatrix(test[feature_names].values)\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "# Try different parameters! My favorite is random search :)\n",
    "xgb_pars = {'min_child_weight': 50, 'eta': 0.3, 'colsample_bytree': 0.3, 'max_depth': 10,\n",
    "            'subsample': 0.8, 'lambda': 1., 'nthread': -1, 'booster' : 'gbtree', 'silent': 1,\n",
    "            'eval_metric': 'rmse', 'objective': 'reg:linear'}\n",
    "\n",
    "# You could try to train with more epoch\n",
    "model = xgb.train(xgb_pars, dtrain, 15, watchlist, early_stopping_rounds=2,\n",
    "                  maximize=False, verbose_eval=1)\n",
    "print('Modeling RMSLE %.5f' % model.best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e901ba0d-287e-45fa-9571-58dcf0dcf05b",
    "_uuid": "b897a4d7d74bf0bf8b349fd883c3af1ff0ab19c1"
   },
   "source": [
    "## Extract following features as our RMSLE is not going below 0.38\n",
    "1. pick to drop cluster \n",
    "2. drop to pickup cluster \n",
    "3. no of lefts \n",
    "4. no. of rights ( 3,4 from osrm data)\n",
    "5. check weather in nyc dataset and see if it can be used as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "2c7feccf-439f-449b-8704-64f76c13bb66",
    "_uuid": "5f7ebbd29d0f32839ac290d5760022044e5e99b3"
   },
   "outputs": [],
   "source": [
    "weather = pd.read_csv('/data/data/newyork/weather-data-in-new-york-city-2016/weather_data_nyc_centralpark_2016.csv.gz')\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eb5561ec-7aae-4359-8495-2087550a90fc",
    "_uuid": "a8d4830ec4521f10cff84c95bd945e1d30ab2dfa",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ggplot import *\n",
    "weather.date = pd.to_datetime(weather.date)\n",
    "weather['day_of_year']= weather.date.dt.dayofyear\n",
    "p = ggplot(aes(x='date'),data=weather) + geom_line(aes(y='minimum temperature', colour = \"blue\")) + geom_line(aes(y='maximum temerature', colour = \"red\"))\n",
    "p + geom_point(aes(y='minimum temperature', colour = \"blue\")) #+ stat_smooth(colour='yellow', span=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "059c6edd-93ed-4261-af61-60e59652dd1c",
    "_uuid": "112bb9016ede2e1b2ace0a0362cee4374fd476f9"
   },
   "source": [
    "**Findings**\n",
    "Clearly In Feb, the temperature falls to 0, and it should have affected the rides as when we check the similar time series plot of the date vs trip_duration, we can see that trip_duration for both vendors are much large than usual on this day. Let's check just for fun the snowfall and snow depth's chart. we can see that temp on particular day may have some effect on trip_duration, In my opinion that information is covered in day_of_year variable so including this data as a feature will be relectant. Lets see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "07c2d6bf-cda5-4762-a05e-f3b375ace900",
    "_uuid": "d1660701b61a4bcaaf7dc467ea272e4c5f3b3229",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather['precipitation'].unique()\n",
    "weather['precipitation'] = np.where(weather['precipitation']=='T', '0.00',weather['precipitation'])\n",
    "weather['precipitation'] = list(map(float, weather['precipitation']))\n",
    "weather['snow fall'] = np.where(weather['snow fall']=='T', '0.00',weather['snow fall'])\n",
    "weather['snow fall'] = list(map(float, weather['snow fall']))\n",
    "weather['snow depth'] = np.where(weather['snow depth']=='T', '0.00',weather['snow depth'])\n",
    "weather['snow depth'] = list(map(float, weather['snow depth']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d89ea2df-2be0-4772-84a2-67cbc1ac8128",
    "_uuid": "99d47e2d27c88a36b05533ca2a560ff2fe445755",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_x = weather['date'].values\n",
    "random_y0 = weather['precipitation']\n",
    "random_y1 = weather['snow fall']\n",
    "random_y2 = weather['snow depth']\n",
    "\n",
    "# Create traces\n",
    "trace0 = go.Scatter(\n",
    "    x = random_x,\n",
    "    y = random_y0,\n",
    "    mode = 'markers',\n",
    "    name = 'precipitation'\n",
    ")\n",
    "trace1 = go.Scatter(\n",
    "    x = random_x,\n",
    "    y = random_y1,\n",
    "    mode = 'markers',\n",
    "    name = 'snow fall'\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x = random_x,\n",
    "    y = random_y2,\n",
    "    mode = 'markers',\n",
    "    name = 'snow depth'\n",
    ")\n",
    "\n",
    "data = [trace0, trace1, trace2]\n",
    "py.iplot(data, filename='scatter-mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f7e9dddd-cd0b-43d4-a71e-3c822abca47d",
    "_uuid": "ea79a68495764728e351bd861ab2d03434cbef2e"
   },
   "source": [
    "Findings\n",
    "From the plot above, we can say that trips are taking more time if snow fall or snow depth is more than a certain amount, though, this information will be captured in day_of_year variable but I guess including features like precipitation, snow fall, snow depth, and temperature will help in tree- split while modelling, so we will include these features as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0491837a-71a5-43e4-92d1-e1e49c416a47",
    "_uuid": "7caae0375b6449d357a870a2aed266d066cc7f7e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def freq_turn(step_dir):\n",
    "    \"\"\"function to create dummy for turn type\"\"\"\n",
    "    from collections import Counter\n",
    "    step_dir_new = step_dir.split(\"|\")\n",
    "    a_list = Counter(step_dir_new).most_common()\n",
    "    path = {}\n",
    "    for i in range(len(a_list)):\n",
    "        path.update({a_list[i]})\n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    if 'straight' in (path.keys()):\n",
    "        a = path['straight']\n",
    "        #print(a)\n",
    "    if 'left' in (path.keys()):\n",
    "        b = path['left']\n",
    "        #print(b)\n",
    "    if 'right' in (path.keys()):\n",
    "        c = path['right']\n",
    "        #print(c)\n",
    "    return a,b,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8a55b631-27d7-44bc-9c2d-7211ff431d9e",
    "_uuid": "21d32e6aa52a919828d89eed073301bafbbb97ef",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fr['straight']= 0\n",
    "train_fr['left'] =0\n",
    "train_fr['right'] = 0\n",
    "train_fr['straight'], train_fr['left'], train_fr['right'] = zip(*train_fr['step_direction'].map(freq_turn))\n",
    "train_fr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "421efbc0-e6c3-4632-9c2f-d58f3d737a9e",
    "_uuid": "4c132cc96393ea0ba6048769378d2a82018a3060",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fr_new = train_fr[['id','straight','left','right']]\n",
    "train = pd.merge(train, train_fr_new, on = 'id', how = 'left')\n",
    "#train = pd.merge(train, weather, on= 'date', how = 'left')\n",
    "print(len(train.columns))\n",
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6181aad6-c59c-4905-a3ec-78c01a320ae8",
    "_uuid": "4e3e695d161d365e452295ec9c34e92e0b2ee9de",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['pickup_datetime'] = pd.to_datetime(train['pickup_datetime'])\n",
    "train['date'] = train['pickup_datetime'].dt.date\n",
    "train.head()\n",
    "\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "train = pd.merge(train, weather[['date','minimum temperature', 'precipitation', 'snow fall',\n",
    "                                 'snow depth']], on= 'date', how = 'left')\n",
    "train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "292e1de3-5404-46a8-a7f5-70c2d90f6bcb",
    "_uuid": "71ca30028faeaa615370b1c720dbaa0d4e61d310",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.loc[:,'hvsine_pick_cent_d'] = haversine_(train['pickup_latitude'].values, train['pickup_longitude'].values, train['centroid_drop_lat'].values, train['centroid_drop_long'].values)\n",
    "train.loc[:,'hvsine_drop_cent_p'] = haversine_(train['dropoff_latitude'].values, train['dropoff_longitude'].values, train['centroid_pick_lat'].values, train['centroid_pick_long'].values)\n",
    "\n",
    "test.loc[:,'hvsine_pick_cent_d'] = haversine_(test['pickup_latitude'].values, test['pickup_longitude'].values, test['centroid_drop_lat'].values, test['centroid_drop_long'].values)\n",
    "test.loc[:,'hvsine_drop_cent_p'] = haversine_(test['dropoff_latitude'].values, test['dropoff_longitude'].values, test['centroid_pick_lat'].values, test['centroid_pick_long'].values)\n",
    "\n",
    "print(\"shape of train_features is {}.\".format(len(train.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "df7efc1c-b78b-4b6f-99a4-fcee8c796ba3",
    "_uuid": "022392f648e4b4054be3c6ed52409cfde72b5af8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fr['straight']= 0\n",
    "test_fr['left'] =0\n",
    "test_fr['right'] = 0\n",
    "test_fr['straight'], test_fr['left'], test_fr['right'] = zip(*test_fr['step_direction'].map(freq_turn))\n",
    "test_fr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "48f60ae6-7860-4374-bd81-12ce253557ae",
    "_uuid": "d973282a702b54f9f8e962ecba9f3b893145189e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fr_new = test_fr[['id','straight','left','right']]\n",
    "test = pd.merge(test, test_fr_new, on = 'id', how = 'left')\n",
    "print(len(test.columns))\n",
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9866dc0d-dd8f-4439-ac4d-40550693d358",
    "_uuid": "efd0eab33d87aade0b39a3a75fb5cc249483a68b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'])\n",
    "test['date'] = test['pickup_datetime'].dt.date\n",
    "test['date'] = pd.to_datetime(test['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b0ecd22e-d220-49a1-9377-6ea47aadf004",
    "_uuid": "c3aba0a64948fe2bb57d22b6c28da9fcd9176d77",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test= pd.merge(test, weather[['date','minimum temperature', 'precipitation', 'snow fall', 'snow depth']], on= 'date', how = 'left')\n",
    "feature_names = list(train.columns)\n",
    "print(\"Difference of features in train and test are {}\".format(np.setdiff1d(train.columns, test.columns)))\n",
    "print(\"\")\n",
    "do_not_use_for_training = ['id', 'pickup_datetime', 'dropoff_datetime', 'trip_duration', 'store_and_fwd_flag', 'date']\n",
    "feature_names = [f for f in train.columns if f not in do_not_use_for_training]\n",
    "print(\"We will be using following features for training {}.\".format(feature_names))\n",
    "print(\"\")\n",
    "print(\"Total number of features are {}.\".format(len(feature_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "38a5df32-e2ef-452a-8a98-62c94686bc80",
    "_uuid": "c7feca28ecbd2e683f72343f1fba71bc07dd7d31",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.log(train['trip_duration'].values + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "78c84921-ba79-4b7f-9b03-d42f9090ea72",
    "_uuid": "75a0decc1fbefefb54a601d18662be82c7e9be41",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtr, Xv, ytr, yv = train_test_split(train[feature_names].values, y, test_size=0.2, random_state=1987)\n",
    "dtrain = xgb.DMatrix(Xtr, label=ytr)\n",
    "dvalid = xgb.DMatrix(Xv, label=yv)\n",
    "dtest = xgb.DMatrix(test[feature_names].values)\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "xgb_par = {'min_child_weight': 20, 'eta': 0.05, 'colsample_bytree': 0.5, 'max_depth': 15,\n",
    "            'subsample': 0.9, 'lambda': 2.0, 'nthread': -1, 'booster' : 'gbtree', 'silent': 1,\n",
    "            'eval_metric': 'rmse', 'objective': 'reg:linear'}\n",
    "\n",
    "model_1 = xgb.train(xgb_par, dtrain, 10, watchlist, early_stopping_rounds=4, maximize=False, verbose_eval=1)\n",
    "print('Modeling RMSLE %.5f' % model.best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0b20895c-9654-4868-a615-434afaecb6d4",
    "_uuid": "532e7d1fb672f44dcdba67a697e31acc36a3f2b3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Modeling RMSLE %.5f' % model_1.best_score)\n",
    "ytest = model_1.predict(dtest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
